
%
% File eacl2021.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{eacl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{bm}

\usepackage{lipsum}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{What Does it Mean? A Challenging Benchmark for Evaluating Word Understanding of Language Models}

\author{Lütfi Kerem Şenel \\
  Center for Information and Language Processing (CIS), LMU Munich, Germany \\
%   \texttt{lksenel@cis.lmu.de} \\\And
%   Timo Shick \\
%   Center for Information and Language Processing (CIS), LMU Munich, Germany \\\And
  Hinrich Schütze \\
    Center for Information and Language Processing (CIS), LMU Munich, Germany \\
  \texttt{inquiries@cislmu.org} \\
  }

\date{}

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{1cm}\arabic{notecounter} $<<<$ ##1: ##2
$>>>$\hspace{1cm}}}}}
\enoteson
\enotesoff

\def\uprm#1#2{\mbox{$_#2^{\hbox{\scriptsize #1}}$}}

\begin{document}
\maketitle
\begin{abstract}

Recent progress in pretraining language models on large
corpora has resulted in significant performance gains on many NLP
tasks. These large models acquire linguistic knowledge
during pretraining, which helps to improve
performance on downstream tasks via fine-tuning. To assess
what kind of knowledge is acquired,
language models are commonly probed by querying them with
`fill in the blank' style cloze questions. Existing probing
datasets mainly focus on knowledge about \emph{relations between}
words and entities. We introduce WDLMPro (Word Definitions
Language Model Probing) to \emph{evaluate word understanding directly} using definitions of
the words. In our experiments, three popular pretrained
language models
struggle to match words and their  definitions. This
indicates that they understand many words poorly and that
our new probing task is a difficult challenge that could
help guide research on LMs in the future.
\end{abstract}



\section{Introduction}

% Maybe it is better to remove the first paragraph because of space restrictions?
Natural language processing (NLP)  has advanced drastically
in the last decade with the design of larger and more
sophisticated models, availability of larger
corpora and increasing computational
power. Pretrained word embeddings
\cite{mikolov13word2vec_b, pennington14glove} popularized
the use of distributed word representations, which became a
fundamental building block for NLP systems. Static word
embeddings were followed by, generally LSTM based, deep
contextual representations of words that are learned on
large corpora via unsupervised language modelling objective
\cite{peters18ELMO}. Large performance gains are obtained
by fine-tuning these models on particular tasks after
unsupervised pretraining \cite{radford18fineTuning,
  howard18ULMFiT}. More recently, attention based
transformer architecture is shown to use context more
effectively \cite{vaswani17transformers} and several
subsequent models achieved state of the art results in many
NLP tasks by combining transformer architecture with
unsupervised pretraining and task specific fine-tuning
\cite{devlin19BERT, liu19RoBERTa}. \newcite{Radford19GPT2}
showed that language models can be applied to a variety of
tasks without task specific fine tuning. This is demonstrated on a much larger scale by \newcite{brown20GPT3}. 

% \enote{hs}{intro up to here: can probably be cut (a little bit too detailed about stuff everybody knows)}

Despite the performance improvements on downstream tasks
provided by the distributed representations and deep neural
models, what they actually learn about language and word meaning  is not clear due to their uninterpretable nature. For static word embeddings, researchers used word similarity \cite{hill15simlex} and word analogy \cite{gladkova16analogy} tests to shed light on what information is captured in these dense vector spaces. For language models, a great amount of linguistic knowledge is stored in the model parameters \cite{peters18dissecting}.  Several studies proposed using `fill in the blank' type cloze statements to test the knowledge learned by these models during unsupervised pretraining. \newcite{petroni19LMasKB} proposed the LAMA (LAnguage Model Analysis) probe to test the factual and common sense knowledge stored in a language model. A similar approach is used in \cite{Schick20rareWords} where authors introduced WNLaMPro (WordNet Language Model Probing) in order to assess the ability of language models to understand words based on their frequency. In WNLaMPro, cloze style questions are generated based on antonym, hypernym and cohyponym relations among words extracted from WordNet along with word corruptions.

The existing probing datasets mainly focus on investigating
the knowledge about relations between words or
entities. However, a more direct way of testing whether a
language model understands the meaning of a word is to
use its dictionary definition. If a pretrained
language model truly understands the meaning of a word, then
it should be able to
match a word with its dictionary definition. 
Based on this motivation, in
this study we introduce the \textit{Word Definitions Language
  Model Probing} (WDLMPro) dataset; it is a challenging
benchmark for testing NLP models for their ability to
understand words.
WDLMPro is essentially a set of
thousands of synset groups; each synset group consists
of a target word (with definition) and its taxonomic sisters
(each with its definition) that have some similarity to the target.
We evaluate two masked language models, BERT and
RoBERTa, and the auto-regressive model GPT-2 on WDLMPro using two different probing tests.
We find that all three models have great difficulty matching
words and their definitions, implying a poor understanding
of word meaning.

% \enote{hs}{we could be more specific about the two different
%   tasks here if there is space:
% }

\section{WDLMPro}
We introduce  WDLMPro (Word Definitions Language Model
Probing) to test how well NLP models can match
nouns and verbs with  their
definitions.
We view this as a test of how well the models understand lexical meaning.
In this section, we
present a detailed description of WDLMPro.

\subsection{Dataset}
WordNet \cite{miller95wordnet} is the basis for
constructing WDLMPro.  A WordNet \textit{synset} contains a
set of synonyms along with a short definition of
the synset.  Different senses of polysemous words are
represented in different synsets providing
disambiguation. WordNet connects synsets with each other via
semantic relations.

Based on a \emph{target synset}
$t$ and the semantic relation hyponymy $<$, we construct
a \emph{synset group} $\cal G$ for the target as follows.
\[
  {\cal G}(t) = \{ x | \exists y: t<y \wedge x<y \}
  \]
that is, {\cal G} contains all synsets that are ``syster
hyponyms'' to $t$ with respect to a hypernym of $t$.
${\cal G}(t)$, along with the definitions of the synsets in
${\cal G}(t)$,
will be used to
set up the WDLMPro tasks that require matching of words and definitions.
We discard groups ${\cal G}(t)$ that
have a size of less than 5.


In this study, we focus on nouns and verbs, i.e., we create
synset groups ${\cal G}$ for the nouns and verbs in WordNet.
Table
\ref{tab:dataset_samples} displays five members from ${\cal
  G} (t)$ 
and their definitions
for the targets
\emph{a\_cappella\_singing.n.01} and
\emph{beckon.v.01}. 
Table \ref{tab:dataset_stats} shows statistics of the dataset.

% \enote{hs}{yes a figure would be nice
% %Maybe a figure to show this process clearer?
% }

% \enote{ks}{
% I skipped the figure for now. 
% }
  
\begin{table}
\centering
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{Masked Language Model (MLM)}} \\\hline
\multirow{3}{*}{\textbf{Noun}} & \underline{\hspace{3mm}} is \texttt{<DEF>} \\
 & \underline{\hspace{3mm}} means \texttt{<DEF>}  \\% \texttt{<DEF>} is the definition of a \underline{\hspace{3mm}} \\
& \underline{\hspace{3mm}} is defined as \texttt{<DEF>} \\\hline
 \multirow{2}{*}{\textbf{Verb}} & definition of \underline{\hspace{3mm}} is to \texttt{<DEF>} \\
 & to \texttt{<DEF>} is the definition of \underline{\hspace{3mm}}  \\ \hline\hline
 \multicolumn{2}{c}{\textbf{Autoregressive Language Model (ALM)}}\\ \hline
\multirow{1}{*}{\textbf{Noun}} & \texttt{<DEF>} is the definition of \underline{\hspace{3mm}} \\
  \hline %\texttt{<DEF>} is the definition of an
 %\underline{\hspace{3mm}}\\ \hline
 \multirow{1}{*}{\textbf{Verb}} &   to \texttt{<DEF>} is the definition of \underline{\hspace{3mm}} \\
\end{tabular}
\caption{Patterns used for querying language models for
  nouns and verbs.
\texttt{<DEF>} refers to the definition,
\underline{\hspace{3mm}} is the mask or missing word that
the language model has to predict.}
\label{tab:patterns}
\end{table}

\begin{table*}
    \centering
    \begin{tabular}{l|l}
    \hline
    \textbf{synset} & \textbf{definition} \\ \hline
    \emph{a\_cappella\_singing.n.01} & \emph{singing without instrumental accompaniment} \\
     caroling.n.01 & singing joyful religious songs (especially at Christmas) \\
     crooning.n.01 & singing in a soft low tone \\
     singalong.n.01 & informal group singing of popular songs \\
     bel\_canto.n.01 & a style of operatic singing \\ \hline
     \emph{beckon.v.01} & \emph{signal with the hands or nod} \\
     applaud.v.01 & clap one's hands or shout after performances to indicate approval \\
     bow.v.01 & bend one's knee or body, or lower one's head \\
     shrug.v.01 & raise one's shoulders to indicate indifference or resignation \\
     exsert.v.01 & thrust or extend out \\ \hline
   
    \end{tabular}
    \caption{Sample candidates for the synset groups with targets \emph{a\_cappella\_singing.n.01} and \emph{beckon.v.01} and their corresponding definitions.} 
    \label{tab:dataset_samples}
\end{table*}


\begin{table}
    \centering
    \begin{tabular}{l|cc}
    \hline
         & \textbf{Noun} & \textbf{Verb} \\ \hline
         \textbf{\# of Synset Groups} & 51559 & 8602 \\
         \textbf{Average \# of Candidates} & 50.2 & 47.7 \\
         \textbf{min / max \# of Candidates} & 5 / 404 & 5 / 593 \\ \hline
    \end{tabular}
    \caption{WDLMPro statistics}
    \label{tab:dataset_stats}
\end{table}

\subsection{Probing Tests}

We define two probing tests that are converses of each other:
\begin{itemize}
  \item \textbf{Match definition to word (D2W).} Given a
    definition and a set of words, the task is to find the
    word that the definition defines.
  \item \textbf{Match word to definition (W2D).} Given a
    word and a set of definitions, the task is to find the
    definition that defines the word.
    \end{itemize}
Each synset group ${\cal G}(t)$ gives rise to one instance
of D2W by providing the definition of $t$, and all words in
${\cal G}(t)$. The word from ${\cal G}(t)$ that matches the
definition has then to be identified. (Note that $t$ is a
member of ${\cal G}(t)$.)
Similarly, each synset group ${\cal G}(t)$ gives rise to one instance
of W2D by providing $t$ and the definitions of all words in
${\cal G}(t)$. The
correct definition  of $t$ has then to be identified among
all definition  candidates. Note that WordNet definitions by
construction do not contain the word to be defined. So there
are no instances where the two tasks are trivial.

\subsubsection{Application to language models}
In principle, any NLP model can be tested on D2W and W2D.
In this paper, we are particularly
interested in testing language models. To this end, we 
convert the data to a format that is suitable for language
models, i.e., to cloze-style questions as
shown in Table
\ref{tab:patterns}. The basic quantity that allows us to
assess the compatibility of a word $t$ and a definition is the
probability of
$t$ being generated for 
``\underline{\hspace{3mm}}'' when the definition is substituted for $<$DEF$>$.

More precisely, we compute the probability that the string
representation of $t$ is being generated.
We will denote the string representation of synset $t$ by
$\bm{t}$. 
We obtain the string representation by removing
the word type and sense information from the name of the
synset and replacing underscores with white space. For
example, synset \emph{miscalculation.n.01}
(\emph{warm\_up.v.04})
is represented by
the string ``miscalculation'' (``warm
up'').

Table \ref{tab:patterns} shows that we define different templates for masked and autoregressive language models.
For the cases with multiple patterns, we average the prediction scores accross patterns and 
% We define alternatives (e.g., ``definition of \underline{\hspace{3mm}}'', ``definition of a \underline{\hspace{3mm}}'', ``definition of an \underline{\hspace{3mm}}'') and will then use, for each word, the alternative that gives the best result. 
% This allows us to generate  WDLMPro automatically as opposed to having to manually postedit a large dataset.
% This is justified because we are interested in how well the model understands the word, not the pattern (cf.\  \newcite{Schick20rareWords}). 

% \enote{hs}{the following should be more formal -- we can
%   probably use Timo's notation}


\subsection{Baselines}

% \enote{hs}{below: we should probably use formal notation for
%   both cases}
% \enote{ks}{Is it formal enough with the equations?} 
  
In this study, we focus on masked  and
autoregressive language models. For a masked language model
$M$, the probability of a candidate $c \in {\cal G}(t)$ on W2D  is calculated as:
\begin{equation*}
  P\uprm{W2D}{M} (c|t) = \prod_{i=1}^{|\bm{t}|}P(\bm{t}^i|Q(c,|\bm{t}|))
\end{equation*}
where $\bm{t} = [\bm{t}^1, \bm{t}^2,...,\bm{t}^{|\bm{t}|}]$
is the tokenization produced by  $M$. $Q(c,|\bm{t}|)$ is the
input query
created from one of the patterns (Table \ref{tab:patterns})
with \underline{\hspace{3mm}} replaced with
$|\bm{t}|$ consecutive mask tokens. For an autoregressive
language model $A$, we
decompose $P(\bm{t}^i|Q(c),\bm{t})$ in the standard way:
\begin{equation*}
    P\uprm{W2D}{A} = \prod_{i=1}^{|\bm{t}|}P(\bm{t}^i|Q(c),\bm{t}^{1},...,\bm{t}^{i-1})
\end{equation*}

For D2W, we need to compare, given a definition, the probabilities of
different candidate words that are generally of different lengths.
To ensure a fair comparison,
we follow
\newcite{Xiong20Encyclopedia}. For masked language models, we
match the number of mask tokens in an input query to the
token count of each candidate. The final score is the average log-probability of the masked tokens:
\begin{equation*}
    P\uprm{D2W}{M}(c|t) =
    \frac{1}{|\bm{c}|}\sum_{i=1}^{|\bm{c}|}\log P(\bm{c}^i |
    Q(t,|\bm{c}|))
\end{equation*}
For autoregressive models, we use the probability of only
the first token:
\begin{equation*}
    P\uprm{D2W}{A}(c|t) =
     P(\bm{c}^1 |
    Q(t))
\end{equation*}
Considering further tokens does not make sense since they
are often easily predictable from the first token.

%for each candidataae since the generation of further tokens will depend on the previously generated tokens, preventing a fair comparison between candidates with different token counts:

We apply our probing test to two different pretrained
masked language models (BERT and
RoBERTa) and one auto-regressive language model
(GPT-2). 
In order to investigate the effect of model
size on the performance, we experiment with both
base and large versions of BERT and RoBERTa
along with all four sizes of GPT-2 (small, medium,
large, xl). 
For RoBERTa, we capitalize the first letter of the candidate
noun word since pretrained RoBERTa models are case sensitive
and expect a capital letter at the beginning of a
sentence.\footnote{Not using capitalization  resulted in
  poor performance for single token target words for D2W.}

\subsection{Measures}
We use two measures: precision at 1 (P@1) and a rank score
(RS), both based on a ranked results list, either of words
or of definitions. P@1 is the percentage of top-ranked items
that is correct.
We define RS as follows:  
\begin{equation*}
    \text{RS}(M,k) = \frac{M-k}{M-1}
\end{equation*}
where $M=|{\cal G}(t)|$ is the number of candidates  and
$k$ is the rank of the correct item, $1 \leq k \leq M$.
Table \ref{tab:dataset_stats} shows that the size of ${\cal G}(t)$
is highly variable;
in contrast to P@1, RS is less affected by
this and the random baseline (cf.\ Tables
\ref{tab:results_find_the_definition}
and \ref{tab:results_find_the_word}) is always 0.5.  


\enote{hs}{i think this may a good paragraph we can cut
  
The first measure we use to measure the performance of
pretrained language models on WDLMPro is precision at 1 (P@1) that measures the performance on successfully finding the target among the candidates. 
However, as shown in Table \ref{tab:dataset_stats}, the size of ${\cal G}(t)$ (number of candidates) varies greatly across synset groups making the resulting precision scores harder to interpret. 
To provide some intuition, we provide the baseline theoretic
P@1 scores for random ranking of the candidates. 
Another disadvantage of the P@1 measure is that it does not consider the rank of the target among the candidates. 
In other words, putting the target at the second rank is regarded as bad as putting it at the last rank. Note that calculating precision at a higher value will be less informative since many synset groups have only 5 samples. 

}

% \enote{hs}{below: it's better to describe the measure in
%   general, not for an example}
\section{Results}

Tables \ref{tab:results_find_the_definition} and 
\ref{tab:results_find_the_word} present
W2D and D2W
results for
BERT, RoBERTa and GPT-2. Performance is clearly better than the
random baseline.
Larger models perform generally better than smaller
ones in both tasks as expected. For W2D,
best performance is achieved by GPT-2$^{xl}$  for 
nouns (53.1\% P@1, 0.85 RS) and by RoBERTa large
for  verbs (54.4\% P@1, 0.88 RS). Performance on D2W is
much lower than for W2D for all
models.  Similar to W2D, GPT-2$^{xl}$ achieves the
best results on D2W for  nouns (39.9\% P@1, 0.81 RS)
and  RoBERTa  for  verbs
(45.3\% P@1, 0.83 RS).

\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
        \hline
         \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Noun}} & \multicolumn{2}{c}{\textbf{Verb}} \\
         & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} \\ \hline
     Bert$_{b}$ & 43.7 & 0.82 & 40.7 & 0.82 \\
     Bert$_{l}$ & 44.3 & 0.83 & 39.0 & 0.81 \\
     Roberta$_{b}$ & 45.5 & 0.83 & 49.1 & 0.85 \\
     Roberta$_{l}$ & 50.0 & 0.85 & 54.4 & 0.88 \\ \hline
     GPT-2$_{s}$ & 44.7 & 0.81 & 43.0 & 0.80 \\
     GPT-2$_{m}$ & 47.6 & 0.82 & 42.3 & 0.80 \\
     GPT-2$_{l}$ & 51.7 & 0.84 & 46.7 & 0.83 \\
     GPT-2$_{xl}$ & 53.1 & 0.85 & 47.0 & 0.83 \\ \hline 
     Random & 7.6 & 0.50 & 7.8 & 0.50 \\\hline
     
    \end{tabular}
    \caption{Precision at 1 and rank score on W2D}
    \label{tab:results_find_the_definition}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
        \hline
         \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Noun}} & \multicolumn{2}{c}{\textbf{Verb}} \\
         & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} \\ \hline
     Bert$_{b}$ & 30.4 & 0.73 & 33.8 & 0.80 \\
     Bert$_{l}$ & 32.6 & 0.75 & 34.0 & 0.81 \\
     Roberta$_{b}$ & 33.3 & 0.75 & 36.0 & 0.78 \\
     Roberta$_{l}$ & 35.7 & 0.76 & 45.3 & 0.83 \\ \hline
     GPT-2$_{s}$ & 34.2 & 0.78 & 27.7 & 0.69 \\
     GPT-2$_{m}$ & 36.2 & 0.79 & 24.8 & 0.69 \\
     GPT-2$_{l}$ & 39.0 & 0.80 & 29.0 & 0.72 \\
     GPT-2$_{xl}$ & 39.9 & 0.81 & 30.3 & 0.73 \\ \hline 
     Random & 7.6 & 0.50 & 8.0 & 0.50 \\\hline
     
    \end{tabular}
    \caption{Precision at 1 and rank score on D2W}
    \label{tab:results_find_the_word}
\end{table}


We also investigate the effect of frequency
on the models' understanding of word meaning.
To this end, we stratify words into 
\textit{rare} (fewer than 10 occurrences),
 \textit{medium} (10 to 99 occurrences) and
 \textit{frequent}
 (100 or more occurrences), based on occurrences in WWC
 (Westbury Wikipedia Corpus, \newcite{WWC}), where we use
 WWC frequency as a substitute for the corpora the models were
 trained on.
 Targets 
 that have more than 3 tokens
(based on NLTK tokenization)
 are taken as rare without
  counting.
We
focus  on  nouns since most  verbs in our dataset
are relatively frequent. Table
\ref{tab:freq_results_find_the_definition} shows that, for W2D,
all models have a poor understanding of the meaning of 
rare and medium words. Moreover, even for 
frequent words, P@1 is below 60\%,  demonstrating that even  large and sophisticated language
models fail to fully understand the meaning of many
nouns.
See appendix for D2W results.

\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
    \hline
         \textbf{Model} & \multicolumn{1}{c}{\textbf{rare}} & \multicolumn{1}{c}{\textbf{medium}} & \multicolumn{1}{c}{\textbf{frequent}} & \multicolumn{1}{c}{\textbf{all}} \\ \hline
     Bert$_{b}$ & 33.7 & 39.2 & 49.7 & 43.7 \\
     Bert$_{l}$ & 32.8 & 38.7 & 51.3 & 44.3 \\
     Roberta$_{b}$ & 38.0 & 42.2 & 49.9 & 45.5 \\
     Roberta$_{l}$ & 40.6 & 45.2 & 55.9 & 50.0 \\ \hline
     GPT-2$_{s}$ & 37.2 & 41.1 & 49.3 & 44.7 \\
     GPT-2$_{m}$ & 38.5 & 43.0 & 53.3 & 47.6 \\
     GPT-2$_{l}$ & 41.8 & 47.3 & 57.7 & 51.7 \\
     GPT-2$_{xl}$ & 42.2 & 48.4 & 59.5 & 53.1 \\ \hline
     Random & 6.6 & 7.0 & 8.2 & 7.6 \\ \hline 
     
    \end{tabular}
    \caption{P@1 scores of W2D for nouns based on target word frequency.}
    \label{tab:freq_results_find_the_definition}
\end{table}



\section{Conclusion}
We introduced
WDLMPro (Word Definitions Language Model
Probing),
a
probing tests that helps us analyze  how well a model
understands word meaning. This is complementary to existing
probing tests that investigate the knowledge of
relations between words or entities.
WDLMPro is based on WordNet word definitions.
We evaluated three popular pretrained language
models on the W2D (word to definition) and D2W (definition
to word) tasks that we define in WDLMPro. Our findings show
that, despite their remarkable performance on many
downstream tasks, these models struggle to match a word and
its true definition, suggesting an insufficient
understanding of word meaning. WDLMPro provides an important
evaluation benchmark, encouraging design and training of
models with precise word understanding.

\bibliography{definition_benchmark}
\bibliographystyle{acl_natbib}

\newpage

put this table in the appendix


\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
    \hline
        \textbf{Model} & \multicolumn{1}{c}{\textbf{rare}} & \multicolumn{1}{c}{\textbf{medium}} & \multicolumn{1}{c}{\textbf{frequent}} & \multicolumn{1}{c}{\textbf{all}} \\ \hline
     Bert$_{b}$ & 20.3 & 26.6 & 36.2 & 30.4 \\
     Bert$_{l}$ & 18.4 & 26.3 & 41.0 & 32.6 \\
     Roberta$_{b}$ & 23.6 & 30.1 & 38.6 & 33.3 \\
     Roberta$_{l}$ & 23.8 & 31.7 & 42.3 & 35.7 \\ \hline
     GPT-2$_{s}$ & 26.5 & 30.4 & 39.0 & 34.2 \\
     GPT-2$_{m}$ & 26.0 & 30.5 & 42.8 & 36.2 \\
     GPT-2$_{l}$ & 28.2 & 33.2 & 45.9 & 39.0 \\
     GPT-2$_{xl}$ & 27.3 & 33.0 & 47.9 & 39.9 \\ \hline
     Random & 6.7 & 7.1 & 8.3 & 7.6 \\ \hline 
     
    \end{tabular}
    \caption{P@1 scores on D2W for  nouns based on target word frequency.}
    \label{tab:freq_results_find_the_word}
\end{table}


\end{document}
