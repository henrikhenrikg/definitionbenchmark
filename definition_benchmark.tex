
%
% File eacl2021.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{eacl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{bm}

\usepackage{lipsum}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{What Does it Mean? A Challenging Benchmark for Evaluating Word Understanding of Language Models}

\author{Lütfi Kerem Şenel \\
  Center for Information and Language Processing (CIS), LMU Munich, Germany \\
%   \texttt{lksenel@cis.lmu.de} \\\And
%   Timo Shick \\
%   Center for Information and Language Processing (CIS), LMU Munich, Germany \\\And
  Hinrich Schütze \\
    Center for Information and Language Processing (CIS), LMU Munich, Germany \\
  \texttt{inquiries@cislmu.org} \\
  }

\date{}

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{1cm}\arabic{notecounter} $<<<$ ##1: ##2
$>>>$\hspace{1cm}}}}}
\enoteson
%\enotesoff

\def\uprm#1#2{\mbox{$_#2^{\hbox{\scriptsize #1}}$}}

\begin{document}
\maketitle
\begin{abstract}

Recent progress in pretraining language models on large
corpora has resulted in significant performance gains on many NLP
tasks. These large models acquire linguistic knowledge
during pretraining, which helps to improve
performance on downstream tasks via fine-tuning. To assess
what kind of knowledge is acquired,
language models are commonly probed by querying them with
`fill in the blank' style cloze questions. Existing probing
datasets mainly focus on knowledge about \emph{relations between}
words and entities. We introduce WDLMPro (Word Definitions
Language Model Probing) to \emph{evaluate word understanding directly} using definitions of
the words. In our experiments, three popular pretrained
language models
struggle to match words and their  definitions. This
indicates that they understand many words poorly and that
our new probing task is a difficult challenge that could
help guide research on LMs in the future.
\end{abstract}



\section{Introduction}

% Maybe it is better to remove the first paragraph because of space restrictions?
Natural language processing (NLP)  has advanced drastically
in the last decade with the design of larger and more
sophisticated models, availability of larger
corpora and increasing computational
power. Pretrained word embeddings
\cite{mikolov13word2vec_b, pennington14glove} popularized
the use of distributed word representations, which became a
fundamental building block for NLP systems. Static word
embeddings were followed by, generally LSTM based, deep
contextual representations of words that are learned on
large corpora via unsupervised language modelling objective
\cite{peters18ELMO}. Large performance gains are obtained
by fine-tuning these models on particular tasks after
unsupervised pretraining \cite{radford18fineTuning,
  howard18ULMFiT}. More recently, attention based
transformer architecture is shown to use context more
effectively \cite{vaswani17transformers} and several
subsequent models achieved state of the art results in many
NLP tasks by combining transformer architecture with
unsupervised pretraining and task specific fine-tuning
\cite{devlin19BERT, liu19RoBERTa}. \newcite{Radford19GPT2}
showed that language models can be applied to a variety of
tasks without task specific fine tuning. This is demonstrated on a much larger scale by \newcite{brown20GPT3}. 

% \enote{hs}{intro up to here: can probably be cut (a little bit too detailed about stuff everybody knows)}

Despite the performance improvements on downstream tasks
provided by the distributed representations and deep neural
models, what they actually learn about language and word meaning  is not clear due to their uninterpretable nature. For static word embeddings, researchers used word similarity \cite{hill15simlex} and word analogy \cite{gladkova16analogy} tests to shed light on what information is captured in these dense vector spaces. For language models, a great amount of linguistic knowledge is stored in the model parameters \cite{peters18dissecting}.  Several studies proposed using `fill in the blank' type cloze statements to test the knowledge learned by these models during unsupervised pretraining. \newcite{petroni19LMasKB} proposed the LAMA (LAnguage Model Analysis) probe to test the factual and common sense knowledge stored in a language model. A similar approach is used in \cite{Schick20rareWords} where authors introduced WNLaMPro (WordNet Language Model Probing) in order to assess the ability of language models to understand words based on their frequency. In WNLaMPro, cloze style questions are generated based on antonym, hypernym and cohyponym relations among words extracted from WordNet along with word corruptions.

The existing probing datasets mainly focus on investigating
the knowledge about relations between words or
entities. However, a more direct way of testing whether a
language model understands the meaning of a word is to
use its dictionary definition. If a pretrained
language model truly understands the meaning of a word, then
it should be able to
match a word with its dictionary definition. 
Based on this motivation, in
this study we introduce the \textit{Word Definitions Language
  Model Probing} (WDLMPro) dataset; it is a challenging
benchmark for testing NLP models for their ability to
understand words.
WDLMPro is essentially a set of
thousands of synset groups; each synset group consists
of a target word (with definition) and its taxonomic sisters
(each with its definition) that have some similarity to the target.
We evaluate two masked language models, BERT and
RoBERTa, and the auto-regressive model GPT-2 on WDLMPro using two different probing tests.
We find that all three models have great difficulty matching
words and their definitions, implying a poor understanding
of word meaning.

% \enote{hs}{we could be more specific about the two different
%   tasks here if there is space:
% }

\section{WDLMPro}
We introduce  WDLMPro (Word Definitions Language Model
Probing) to test how well NLP models can match
nouns and verbs with  their
definitions.
We view this as a test of how well the models understand lexical meaning.
In this section, we
present a detailed description of WDLMPro.

\subsection{Dataset}
WordNet \cite{miller95wordnet} is the basis for
constructing WDLMPro.  A WordNet \textit{synset} contains a
set of synonyms along with a short definition of
the synset.  Different senses of polysemous words are
represented in different synsets providing
disambiguation. WordNet connects synsets with each other via
semantic relations.

Based on a \emph{target synset}
$t$ and the semantic relation hyponymy $<$, we construct
a \emph{synset group} $\cal G$ for the target as follows.
\[
  {\cal G}(t) = \{ x | \exists y: t<y \wedge x<y \}
  \]
that is, {\cal G} contains all synsets that are ``syster
hyponyms'' to $t$ with respect to a hypernym of $t$.
${\cal G}(t)$, along with the definitions of the synsets in
${\cal G}(t)$,
will be used to
set up the WDLMPro tasks that require matching of words and definitions.
We discard groups ${\cal G}(t)$ that
have a size of less than 5.


In this study, we focus on nouns and verbs, i.e., we create
synset groups ${\cal G}$ for the nouns and verbs in WordNet.
Table
\ref{tab:dataset_samples} displays five members from ${\cal
  G} (t)$ 
and their definitions
for the targets
\emph{a\_cappella\_singing.n.01} and
\emph{beckon.v.01}. 
Table \ref{tab:dataset_stats} shows statistics of the dataset.

% \enote{hs}{yes a figure would be nice
% %Maybe a figure to show this process clearer?
% }

% \enote{ks}{
% I skipped the figure for now. 
% }
  
\begin{table*}
    \centering
    \begin{tabular}{l|l}
    \hline
    \textbf{synset} & \textbf{definition} \\ \hline
    \emph{a\_cappella\_singing.n.01} & \emph{singing without instrumental accompaniment} \\
     caroling.n.01 & singing joyful religious songs (especially at Christmas) \\
     crooning.n.01 & singing in a soft low tone \\
     singalong.n.01 & informal group singing of popular songs \\
     bel\_canto.n.01 & a style of operatic singing \\ \hline
     \emph{beckon.v.01} & \emph{signal with the hands or nod} \\
     applaud.v.01 & clap one's hands or shout after performances to indicate approval \\
     bow.v.01 & bend one's knee or body, or lower one's head \\
     shrug.v.01 & raise one's shoulders to indicate indifference or resignation \\
     exsert.v.01 & thrust or extend out \\ \hline
   
    \end{tabular}
    \caption{Sample candidates for the synset groups with targets \emph{a\_cappella\_singing.n.01} and \emph{beckon.v.01} and their corresponding definitions.} 
    \label{tab:dataset_samples}
\end{table*}


\begin{table}
    \centering
    \begin{tabular}{l|cc}
    \hline
         & \textbf{Noun} & \textbf{Verb} \\ \hline
         \textbf{\# of Synset Groups} & 51559 & 8602 \\
         \textbf{Average \# of Candidates} & 50.2 & 47.7 \\
         \textbf{min / max \# of Candidates} & 5 / 404 & 5 / 593 \\ \hline
    \end{tabular}
    \caption{WDLMPro statistics}
    \label{tab:dataset_stats}
\end{table}

\subsection{Probing Tests}

We define two probing tests that are converses of each other:
\begin{itemize}
  \item \textbf{Match definition to word (D2W).} Given a
    definition and a set of words, the task is to find the
    word that the definition defines.
  \item \textbf{Match word to definition (W2D).} Given a
    word and a set of definitions, the task is to find the
    definition that defines the word.
    \end{itemize}
Each synset group ${\cal G}(t)$ gives rise to one instance
of D2W by providing the definition of $t$, and all words in
${\cal G}(t)$. The word from ${\cal G}(t)$ that matches the
definition has then to be identified. (Note that $t$ is a
member of ${\cal G}(t)$.)
Similarly, each synset group ${\cal G}(t)$ gives rise to one instance
of W2D by providing $t$ and the definitions of all words in
${\cal G}(t)$. The
correct definition  of $t$ has then to be identified among
all definition  candidates. Note that WordNet definitions by
construction do not contain the word to be defined. So there
are no instances where the two tasks are trivial.

\subsubsection{Application to language models}
In principle, any NLP model can be tested on D2W and W2D.
In this paper, we are particularly
interested in testing language models. To this end, we 
convert the data to a format that is suitable for language
models, i.e., to cloze-style questions as
shown in Table
\ref{tab:patterns}. The basic quantity that allows us to
assess the compatibility of a word $t$ and a definition is the
probability of
$t$ being generated for 
``\underline{\hspace{3mm}}'' when the definition is substituted for $<$DEF$>$.

More precisely, we compute the probability that the string
representation of $t$ is being generated.
We will denote the string representation of synset $t$ by
$\bm{t}$. 
We obtain the string representation by removing
the word type and sense information from the name of the
synset and replacing underscores with white space. For
example, synset \emph{miscalculation.n.01}
(\emph{warm\_up.v.04})
is represented by
the string ``miscalculation'' (``warm
up'').

Table
\ref{tab:patterns}
shows
that we define different templates for masked
and autoregressive language models. We define alternatives
(e.g., ``definition of \underline{\hspace{3mm}}'', 
``definition of a \underline{\hspace{3mm}}'', 
``definition of an \underline{\hspace{3mm}}'') and will then
use, for each word, the alternative that gives the best
result. This allows us to generate  WDLMPro automatically as
opposed to having to manually postedit a large dataset.
This is justified because we are interested in
how well the model understands the word, not the pattern (cf.\  \newcite{Schick20rareWords}). 

% \enote{hs}{the following should be more formal -- we can
%   probably use Timo's notation}

\begin{table*}
\centering
\begin{tabular}{lll}
\hline
& \textbf{Masked Language Model} & \textbf{Autoregressive Language Model}\\ \hline
\multirow{3}{*}{\textbf{Noun}} & \underline{\hspace{3mm}} is \texttt{<DEF>} & \texttt{<DEF>} is the definition of \underline{\hspace{3mm}} \\
 & \underline{\hspace{3mm}} means \texttt{<DEF>} & \texttt{<DEF>} is the definition of a \underline{\hspace{3mm}} \\
 & \underline{\hspace{3mm}} is defined as \texttt{<DEF>} & \texttt{<DEF>} is the definition of an \underline{\hspace{3mm}}\\ \hline
 \multirow{2}{*}{\textbf{Verb}} & definition of \underline{\hspace{3mm}} is to \texttt{<DEF>} & to \texttt{<DEF>} is the definition of \underline{\hspace{3mm}} \\
 & to \texttt{<DEF>} is the definition of \underline{\hspace{3mm}} & \\ \hline
 
\end{tabular}
\caption{Patterns used for querying language models for
  nouns and verbs.
\texttt{<DEF>} refers to the definition,
\underline{\hspace{3mm}} is the mask or missing word that
the language model has to predict.}
\label{tab:patterns}
\end{table*}

\subsection{Baselines}

% \enote{hs}{below: we should probably use formal notation for
%   both cases}
% \enote{ks}{Is it formal enough with the equations?} 
  
In this study, we focus on masked  and
autoregressive language models. For a masked language model
$M$, the probability of a candidate $c \in {\cal G}(t)$ on W2D  is calculated as:
\begin{equation*}
  P\uprm{W2D}{M} (c|t) = \prod_{i=1}^{|\bm{t}|}P(\bm{t}^i|Q(c,|\bm{t}|))
\end{equation*}
where $\bm{t} = [\bm{t}^1, \bm{t}^2,...,\bm{t}^{|\bm{t}|}]$
is the tokenization produced by  $M$. $Q(c,|\bm{t}|)$ is the
input query
created from one of the patterns (Table \ref{tab:patterns})
with \underline{\hspace{3mm}} replaced with
$|\bm{t}|$ consecutive mask tokens. For an autoregressive
language model $A$, we
decompose $P(\bm{t}^i|Q(c),\bm{t})$ in the standard way:
\begin{equation*}
    P\uprm{W2D}{A} = \prod_{i=1}^{|\bm{t}|}P(\bm{t}^i|Q(c),\bm{t}^{1},...,\bm{t}^{i-1})
\end{equation*}

For D2W, we need to compare, given a definition, the probabilities of
different candidate words that are generally of different lengths.
To ensure a fair comparison,
we follow
\newcite{Xiong20Encyclopedia}. For masked language models, we
match the number of mask tokens in an input query to the
token count of each candidate. The final score is the average log-probability of the masked tokens:
\begin{equation*}
    P\uprm{D2W}{M}(c|t) =
    \frac{1}{|\bm{c}|}\sum_{i=1}^{|\bm{c}|}\log P(\bm{c}^i |
    Q(t,|\bm{c}|))
\end{equation*}
For autoregressive models, we use the probability of only
the first token:
\begin{equation*}
    P\uprm{D2W}{A}(c|t) =
     P(\bm{c}^1 |
    Q(t))
\end{equation*}
Considering further tokens does not make sense since they
are often easily predictable from the first token.

%for each candidataae since the generation of further tokens will depend on the previously generated tokens, preventing a fair comparison between candidates with different token counts:

We apply our probing test to two different pretrained
masked language models (BERT and
RoBERTa) and one auto-regressive language model
(GPT-2). 
In order to investigate the effect of model
size on the performance, we experiment with both
base and large versions of BERT and RoBERTa
along with all four sizes of GPT-2 (small, medium,
large, xl). 
For RoBERTa, we capitalize the first letter of the candidate
noun word since pretrained RoBERTa models are case sensitive
and expect a capital letter at the beginning of a
sentence.\footnote{Not using capitalization  resulted in
  poor performance for single token target words for D2W.}

\subsection{Measures}

% \enote{hs}{below: since accuracy or precision at 1 is a
%   standard measure, we probably don't need such a detailed description}
  
The first measure we use to measure the performance of
pretrained language models on WDLMPro is precision at 1 (P@1) that measures the performance on successfully finding the target among the candidates. 
However, as shown in Table \ref{tab:dataset_stats}, the size of ${\cal G}(t)$ (number of candidates) varies greatly across synset groups making the resulting precision scores harder to interpret. 
To provide some intuition, we provide the baseline theoretic
P@1 scores for random ranking of the candidates. 
Another disadvantage of the P@1 measure is that it does not consider the rank of the target among the candidates. 
In other words, putting the target at the second rank is regarded as bad as putting it at the last rank. Note that calculating precision at a higher value will be less informative since many synset groups have only 5 samples. 

% \enote{hs}{below: it's better to describe the measure in
%   general, not for an example}
  
To address the limitations of the accuracy measure, we introduce a \textit{rank score} measure which provides a uniform weighting between 0 and 1 for the possible rankings of the target among the candidates. Rank score is calculated as:
\begin{equation*}
    \text{RS}(M,k) = \frac{M-k}{M-1}
\end{equation*}
where $M$ is the number of candidates in a synset group and $k$ is the predicted rank for a target ($k \in
\{1,...,M\}$). Contrary to the precision measure, rank score is not affected by the number of candidates and the random baseline is always 0.5.  

\section{Results}

Table \ref{tab:results_find_the_definition} and Table
\ref{tab:results_find_the_word} present the results for the
BERT, RoBERTa and GPT-2 models
for W2D and D2W. Performance is clearly better than the
random baseline.
Larger models perform generally better than smaller
ones in both tasks as expected. For W2D,
best performance is achieved by GPT-2$^{xl}$ model for 
nouns (53.1\% P@1, 0.85 rank score) while RoBERTa large
performs the best for  verbs (54.4\% P@1, 0.88 rank
score). Performance on D2W is
significantly lower than for W2D for all
models.  As was the case for W2D, GPT-2$^{xl}$ achieves the
best results on D2W for  nouns (39.9\% P@1, 0.81 rank
score) while RoBERTa large performs best for  verbs
(45.3\% P@1, 0.83 rank score).

\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
        \hline
         \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Noun}} & \multicolumn{2}{c}{\textbf{Verb}} \\
         & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} \\ \hline
     Bert$_{b}$ & 43.7 & 0.82 & 40.7 & 0.82 \\
     Bert$_{l}$ & 44.3 & 0.83 & 39.0 & 0.81 \\
     Roberta$_{b}$ & 45.5 & 0.83 & 49.1 & 0.85 \\
     Roberta$_{l}$ & 50.0 & 0.85 & 54.4 & 0.88 \\ \hline
     GPT-2$_{s}$ & 44.7 & 0.81 & 43.0 & 0.80 \\
     GPT-2$_{m}$ & 47.6 & 0.82 & 42.3 & 0.80 \\
     GPT-2$_{l}$ & 51.7 & 0.84 & 46.7 & 0.83 \\
     GPT-2$_{xl}$ & 53.1 & 0.85 & 47.0 & 0.83 \\ \hline 
     Random & 7.6 & 0.50 & 7.8 & 0.50 \\\hline
     
    \end{tabular}
    \caption{Precision at 1 and rank scores on W2D}
    \label{tab:results_find_the_definition}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
        \hline
         \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Noun}} & \multicolumn{2}{c}{\textbf{Verb}} \\
         & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} & \multicolumn{1}{c}{P@1} & \multicolumn{1}{c}{RS} \\ \hline
     Bert$_{b}$ & 30.4 & 0.73 & 33.8 & 0.80 \\
     Bert$_{l}$ & 32.6 & 0.75 & 34.0 & 0.81 \\
     Roberta$_{b}$ & 33.3 & 0.75 & 36.0 & 0.78 \\
     Roberta$_{l}$ & 35.7 & 0.76 & 45.3 & 0.83 \\ \hline
     GPT-2$_{s}$ & 34.2 & 0.78 & 27.7 & 0.69 \\
     GPT-2$_{m}$ & 36.2 & 0.79 & 24.8 & 0.69 \\
     GPT-2$_{l}$ & 39.0 & 0.80 & 29.0 & 0.72 \\
     GPT-2$_{xl}$ & 39.9 & 0.81 & 30.3 & 0.73 \\ \hline 
     Random & 7.6 & 0.50 & 8.0 & 0.50 \\\hline
     
    \end{tabular}
    \caption{Precision at 1 and rank scores on D2W}
    \label{tab:results_find_the_word}
\end{table}


We also investigate the effect of the frequency of the
target word on model performance. For this part, we
focus only on  nouns since most  verbs in our dataset
are relatively frequent. Because these models are trained on
different corpora, frequency of a target word (or number of
times a model sees a target word during training) can be
different across  models. Nonetheless, we group the
target words based on the number of times they occur in the
Westbury Wikipedia Corpus (WWC) \cite{WWC} as an
approximation. We
define three subsets based on the occurrence counts of the
target words in WWC: \textit{rare} (fewer than 10 occurrences),
 \textit{medium} (10 to 99 occurrences) and
 \textit{frequent}
(100 or more occurrences).\footnote{Both the corpus and
  the target words are tokenized using NLTK tokenizer.  Targets
  that contain more than 3 tokens are taken as rare without
  counting.}. Table
\ref{tab:freq_results_find_the_definition} and Table
\ref{tab:freq_results_find_the_word} present the  results
based on target word frequency for
W2D and D2W.
These results show that
all models have a poor understanding of the meaning of 
rare and medium words. Moreover, even for 
frequent words P@1 scores are below 60\% on W2D and below 50\% on D2W for all models demonstrating that even  large and sophisticated language
models fail to learn nuances in the meaning of many
nouns.

\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
    \hline
         \textbf{Model} & \multicolumn{1}{c}{\textbf{rare}} & \multicolumn{1}{c}{\textbf{medium}} & \multicolumn{1}{c}{\textbf{frequent}} & \multicolumn{1}{c}{\textbf{all}} \\ \hline
     Bert$_{b}$ & 33.7 & 39.2 & 49.7 & 43.7 \\
     Bert$_{l}$ & 32.8 & 38.7 & 51.3 & 44.3 \\
     Roberta$_{b}$ & 38.0 & 42.2 & 49.9 & 45.5 \\
     Roberta$_{l}$ & 40.6 & 45.2 & 55.9 & 50.0 \\ \hline
     GPT-2$_{s}$ & 37.2 & 41.1 & 49.3 & 44.7 \\
     GPT-2$_{m}$ & 38.5 & 43.0 & 53.3 & 47.6 \\
     GPT-2$_{l}$ & 41.8 & 47.3 & 57.7 & 51.7 \\
     GPT-2$_{xl}$ & 42.2 & 48.4 & 59.5 & 53.1 \\ \hline
     Random & 6.6 & 7.0 & 8.2 & 7.6 \\ \hline 
     
    \end{tabular}
    \caption{P@1 scores of W2D for nouns based on target word frequency.}
    \label{tab:freq_results_find_the_definition}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{l|rrrr}
    \hline
        \textbf{Model} & \multicolumn{1}{c}{\textbf{rare}} & \multicolumn{1}{c}{\textbf{medium}} & \multicolumn{1}{c}{\textbf{frequent}} & \multicolumn{1}{c}{\textbf{all}} \\ \hline
     Bert$_{b}$ & 20.3 & 26.6 & 36.2 & 30.4 \\
     Bert$_{l}$ & 18.4 & 26.3 & 41.0 & 32.6 \\
     Roberta$_{b}$ & 23.6 & 30.1 & 38.6 & 33.3 \\
     Roberta$_{l}$ & 23.8 & 31.7 & 42.3 & 35.7 \\ \hline
     GPT-2$_{s}$ & 26.5 & 30.4 & 39.0 & 34.2 \\
     GPT-2$_{m}$ & 26.0 & 30.5 & 42.8 & 36.2 \\
     GPT-2$_{l}$ & 28.2 & 33.2 & 45.9 & 39.0 \\
     GPT-2$_{xl}$ & 27.3 & 33.0 & 47.9 & 39.9 \\ \hline
     Random & 6.7 & 7.1 & 8.3 & 7.6 \\ \hline 
     
    \end{tabular}
    \caption{P@1 scores on D2W for  nouns based on target word frequency.}
    \label{tab:freq_results_find_the_word}
\end{table}


\section{Conclusion}

Probing tests help us understand what information
is stored in the parameters of a model and how well a model
understands a given word or concept. Currently available
probing tests focus on investigating the 
knowledge of relations between words or entities. In this study, we
introduced the WDLMPro (Word Definitions Language Model Probing)
dataset for evaluating word understanding of language models
in a more direct way based on word definitions. We evaluated
three popular pretrained language models on
the W2D (word to definition) and D2W (definition to word)
tasks that we define in
WDLMPro. Our findings show that, despite their remarkable
performance on many downstream tasks, these models struggle
to match a word and its true definition, suggesting an
insufficient understanding of word meaning. WDLMPro provides an
important evaluation benchmark, encouraging design and
training of models with precise word understanding.

\bibliography{definition_benchmark}
\bibliographystyle{acl_natbib}

\end{document}
